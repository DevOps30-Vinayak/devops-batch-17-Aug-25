DevOps Engineer
==================
- Infrastructure (AWS, Terraform)
- Networking commands - Linux 
- Containerization (Deploy application on production server) - Docker , Kubernetes
- Setup CI/CD pipeline to deploy applications (JENKINS)


Environments in a DevOps Lifecycle
----------------------------------------------------------------------------------------------------------------------------------
Development :  Build and verify application components.

Write source code for domains like E-Commerce, Health Care, Banking, Finance.

Example API: http://calc/x/10/y/20.

Perform Unit Tests (check individual functions) and Integration Tests (validate multiple components together).

Collaborate on the shared codebase using GitHub (Version Control, Branching, Pull Requests).


Testing : Ensure application correctness, stability, and performance before production.
Manual Testing → e.g., Postman for API validation.

Automation Testing → e.g., Selenium, Cypress for regression and UI automation.

Performance Testing → e.g., JMeter for load, stress, and scalability testing.

Production : Deploy applications in a reliable, scalable, and secure manner.

Infrastructure Setup → IaaS (Cloud Virtual Machines, Networks)

Use IaC (Infrastructure as Code) tools (Terraform, Ansible, CloudFormation).

Application Deployment → Ensure Scalability, High Availability (HA), Fault Tolerance, and Minimal Downtime, Application start time

Automation & Scaling →
AWS Auto Scaling → automatically scale Virtual Machines (EC2).

Kubernetes Auto Scaling → scale Pods based on CPU/Memory metrics.

Horizontal scaling (adding more servers/instances).

Vertical scaling (adding more resources to existing servers).
   -------------------------------------------------------------------------------------------------------------------------------              
Dev (Code + Build + Test)

Ops (Deploy + Operate + Scale)
------------------------------------------------------------------------------------------------------------------------------
Docker and Kubernetes - Work on Containers

Containers - are used to deployment of applications / to run application with less down time 
                   - portability of applications


Scenario : NodeJS application ? Develop backend APIS in JavaScript
                  - API develop - Node has library - expressJS

You need to run the nodejs application on the local machine.
Copy files from Windows  ------------> Linux
                                       WinSCP utility tool

Implementation:           
1. We need OS (Linux)
2. Software - NODEJS installed, for coding - vim/nano
3. Application structure
     package.json: Contains project metadata, scripts, and dependencies.
     .env: Stores environment variables (e.g., database credentials, API keys). // optional
     .gitignore: Specifies files and directories to be ignored by Git.
      app.js: The main entry point of the application, often responsible for starting the server and setting up middleware.

4. Write the code
5. test the code by executing the application  local machine


 Two ways to install                      
1. package manager suggestion
  - apt update -y
  - apt install nodejs

2. Manual installation
1. Download tar file
2. Extract tar file (tar -xvf...)
3. MV command to move extract folder to /usr/local/lib/nodejs
4. Export the Path of NodeJS


nodeJS - install
source code - app.js , package.json
install the libraries added in package.json  (npm install)
Run the application (node app.js)


28-Sep-25
================================================
Agenda:

What are containers? 
Difference between VM and containers.
Setup Docker on Linux - completed
Docker architecture - completed
Run containers on Docker - Next session


Containers package the application code along with its dependencies (runtime, libraries, configuration, tools) into a single portable unit.
------------------------------------------------------------------------------------------
Containers do not bundle a full OS kernel like a virtual machine.

They share the host OS kernel, but can package minimal OS user-space components (like Ubuntu base image).

That’s why they are lightweight compared to VMs.

Summary - Containers do not include a full operating system kernel. They share the host OS kernel, making them lightweight compared to virtual machines.
----------------------------------------------------------------------------------------------------

Containers have a life cycle. Containers go through states like Created → Running → Stopped → Removed.
---------------------------------------------------------------------------------------------------------
Containers run in isolation using Linux features like namespaces (for process, network, filesystem isolation).
----------------------------------------------------------------------------------------------------------
Each container is assigned resources using Linux cgroups and gets its own network namespace, configured by the container runtime (e.g., Docker).

Objective: Run the node application 
Problems:
1. OS  
Setup the necessary software and dependencies on local machine
2. If application has to run on different machine/ different environments - same setup has to be replicated on every machines
     tools - versions
      os - match

Development Env ----------------------------------> Testing Env -------------------------> Production Env
         App                                                             App                                            App


----------------------------------------------------------------------------------------------------------------------------->
Match: OS/ Software with right versions

3. No portability
4. Application start time is high 


Solution : Run applications in containers 

           
 
Two-Tier appl
ReactJS <---------> NodeJS -------------------------------> MYSQL

1 Container  = 1 Application (Best practice)
1 Container = NODEJS
1 Container = MYSQL


Containers vs VM - https://www.netapp.com/blog/containers-vs-vms/


setup Docker

  - on Windows (Docker Desktop runs on WSL Linux)
  - Install on WSL Linux
  - Install on AWS/ Azure VM
 

  - Install on WSL Linux
  https://docs.docker.com/engine/install/

1. sudo su

1. sudo apt-get update

2. sudo apt-get install ca-certificates curl

3. sudo install -m 0755 -d /etc/apt/keyrings

4. sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc

5. sudo chmod a+r /etc/apt/keyrings/docker.asc

6. echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

7. sudo apt-get update

8. sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

9. test docker
docker

10. exit // from sudo

11. Normal user login - sudo usermod -aG docker $USER

12. Restart the Linux (WSL Ubnbtu) again

14. id
check the docker group

15. docker info
Client:
 ...
Server:
....


Setup an account on Docker HUB to push docker images
username:
password:
personal email id

Docker architecture - https://docs.docker.com/get-started/docker-overview/

Docker CLI commands

docker info
docker images / docker image ls
docker ps // to see running containers
docker container ls // to see list of running containers

see the default network
docker network ls

see the storage of docker
docker volume ls




Docker
   - Containers
  - Images
 - volumes
 - networks
 - docker compose




OOP programming
  Class - a blueprint or template (Set of instructions)
  Objects - Runtime instances

Docker two main components
 Images - a blueprint or template (set of instructions)
 Containers - Runtime instances

Tools to scale containers are primarily container orchestrators such as Kubernetes, Docker Swarm, and AWS ECS. These platforms automate the deployment, scaling.


what is Cgroups in docker?
cgroups (control groups): Used to allocate and limit CPU, memory, etc. for containers.


what are namespaces in Linux?
In Linux, a namespace is a kernel feature that isolates and virtualizes system resources for a set of processes.

It makes processes inside a namespace think they have their own instance of a resource (like PID numbers, network interfaces, or mount points), even though they are actually sharing the host system’s kernel.

04-Oct-25
Containers
  - Containers do not OS
 - Containers are portable - You can run in any env where docker engine. 
    Images are recipes to create the container
 - Containers are isolated from each other

Docker

   - creates images, containers, network, volumes
   Linux (ubuntu) - /var/lib/docker (info about docker components/objects are stored by Docker)

   - Docker  creates virtual network  with IP address - 172.17.0.1 
   - Docker has written network drivers

Type of applications that can run in a containers
---------------------------------------------------------------
Web application (React, Node, Python, Java)
DB Servers (postgres, MySQL, Oracle.., MongodB)
Light-weight Linux OS tools (Debain, Alpine, Busybox, Ubuntu
Tools - Jenkins, Terraform, Promoetheus, Grafana

Installation of Jenkins
Windows -> WSL (Ubuntu) -> update OS -> Install Java, then Install JEnkins -> Access Jenkins server

or

Browser -> Login into AWS -> Create Linux VM ->  Install Java, then Install Jenkins -> Access Jenkins server


With Docker -> Windows -> WSL (ubuntu) -> Docker -> Take Ready image of Jenkins -> Run Container [docker container run <<jenkins image>>] -> Access the application 

Browser -> Login into AWS -> Create Linux VM ->  Docker -> Take Ready image of Jenkins -> Run Container [docker container run <<jenkins image>>] -> Access the application 

Browser -> Login into AWS -> AWS ECS -> Container Technology (Elastics Container service) -> Ready Image -> ECS will run and manage containers




Images ???
1. Write the image and build it 
or
2. Pull the ready image from docker repository (Available on Internet - Docker Hub / AWS - ECR or Azure cloud)


Practicals


docker image ls

or

docker images


Name of Image : <nameofimage>:<tag>

jenkins:2.60.3 // Linux - libraies?
jenkins:alpine  // Linux - alpine libraries
Jenkins:latest // latest is the default tag



docker pull // old command


main command category
image
container
volume
network

sub-command category
pull
push
build


1. Pulled the hello-world image

docker image pull hello-world:latest

or
docker image pull hello-world
  // use default tag : latest

Compares the sha256 for the image to be pulled

SHA-256 (Secure Hash Algorithm 256-bit) is a cryptographic hash function that converts any input data into a fixed-size 256-bit (32-byte) "hash" or "fingerprint". 

docker container run options
 --name : to name a container / -n
-p : assign application port to the host (used for web  app and db)
--attach /--detach
--network - assign network to a container
--volume - assign storage to a container
--env - set the environment variables for the container(app)
--rm - delete the container info


By default, all application output / logs are displayed on the Linux terminal (screen)


Running the container - docker container run hello-world:latest


main(){

printf("hello-world")
for(1 to 100){..}
exit 1
}

console-based app - will terminate once code execution is completed
web app -> based on http, hence waits for client request 

Life of container  = Life of application


docker container ls - running containers
docker ps -a (Exited containers)

Container Life Cycle
1. Verify if the image exists take it from local machine, if not pull the image 
2. The container is created in the memory
3. Run the container 
4. start the application inside the container CMD /hello
5. Display the output of application on the Linux console
6. If application exited, exit the container


Container info
 - container id
- container name 





Images:
Images are made up of  file system changes and metadata
Each layer is unique identified and only stored once on a host
This saves storage space on host and transfer time in push/pull


Linux script

  apt install nginx
  copy <source path>  <destination-path>

Docker recipe (script)

RUN apt install nginx
COPY ./app.js    /usr/app
CMD ["node", "app.js"] // start the application in container



Next step:

docker image build <image-name>

build will create the layers



Scenario: to create a basic image with Debian tools.

 docker image build -t mybasicimg:oct25



GIT - code 
To push image to the docker repository (Dockerhub)

   - create the image tag with dockerhub username
    tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE
    docker image tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]

Tagging the image: The image must be tagged with the full repository name and optionally a tag (e.g., username/repository:tag). This typically involves using the docker tag command.
   - push the image
  

From Linux distro image -> Get only Linux tools, FROM Debian:....
FRom software image , FROM  optimized : nginx:1.29.2-alpine



CMD  -> Change the command to execute in the container at the runtime

docker container run nginx:prod mkdir test

project express - 4.17 -----> Nodejs 14.x


nodeproject'  

    app.js
    package.json


cd nodeproject

nodeproject>ls
app.js
package.json (EXPRESSJS)


nodeproject> npm install

node_modules
  libraries downloaded

nodeproject>  node app.js


Assignment on Docker - DevOps Scenario Question
   - python
   - flask API to create web applications

 Dockerize the python web application


Scenario: Run a MYSQL sever -> data needs to be stored - completed

create a volume for MYSQL Containers - completed

Why volumes - data storage from containers - MYSQL (RDMS), MongoDB (NO SQL DB), Redis, h2 (in-memory db)
  |
Docker creates volumes - /var/lib/docker/volumes (named volumes)
  |
Create the volume - docker volume create <volume-name>
Add a volume to a container: docker container run  -v    mysqldata:/var/lib/mysql     mysql:latest
 Volumes are read-write 

|
if container is destroyed, volumes will still exist

To work with volumes - 

docker volume ls
docker volume create ...
docker volume inspect <volume-name>

Types of volumes in docker
 - Named volumes - volumes created and managed by docker. docker volume create...
 - Bind volumes - when you want to mount your project files in the container at the start
nginx project
    - index.html --------------> NGINX container
                           mount

bind volumes
nginxproject>docker container run --mount type=volume -v ./index.html:/usr/share/nginx/html
docker run --mount type=volume,src=myvolume,dst=/data,ro,volume-subpath=/foo

Named volumes
docker create volume mynginxdata
docker container run -v mynginxdata:/usr/share/nginx/html mynginx:latest

-DBProject
    - dbinitscript ---------------> mounted in MySQL container

 -v abc:/usr/share/nginx/index.html


Scenario for mounting project files from Linux host into the container.


docker container run -d --name web5 -p 9000:80 --mount type=bind,src=.,dst=/usr/share/nginx/html,ro nginx:latest
You
10:28 AM
docker logs web5
docker container inspect web5
check Mount section:
 "Mounts": [
                {
                    "Type": "bind",
                    "Source": "/home/user1/dockerws/bindvolmedemo",
                    "Target": "/usr/share/nginx/html",
                    "ReadOnly": true
                }
            ],
docker container exec -it web5 /bin/bash
You
10:30 AM
Send request to the nginx app - http://localhost:9000/

-------------------------------------------------------------------------------------------------------------------
Docker compose tool

Run container with scripts

Why to use docker compose
  - Manage multiple containers 
  - write configurations of containers in a file named as *.yaml/*.yml
  - Remember, to run containers, we need images

Example:

Node Project  uses database <----------------------> DB
 Container 1                                                    Container 2
 NodeJS                                                           Mongo DB
(Custom image)                                                (Dockerhub)
Dockerfile     
depends_on: name of mongodb started by docker compose

 
 - Nginx server run as container (image can be pulled from Dockerhub)
 - MySQL database server as container (image can be pulled from Dockerhub)
 - Python / NodeJS application as container (in need to build the image)


YAML Syntax
YAML - Yet Another MArkup Language
Web technologies  - HTML (<p>  </p>, <h1> </h1>) - <> called as MArkup
HTML - markup language defined by W3c (W3 - World Wide Web)

Devops Tools - Based on Configurations 
Kubernetes, Docker-compose, Terraform, Ansible - need configurations 

Configurations are written for Devops Tools
Configurations are written in declarative syntax

Declarative syntax - What do you want  ------> Tool

Nginx
  Image:
  Port:
  Network:
  Volume



Imperative Syntax - Linux script/Java/ Node/ Python Code
Imperative Syntax - What do you want + How to do that
Example:

change the permission of ten files

for x in file1 file2 file3
do
  chmod 777 $x

done


Array : skills[3] = {"Docker", "Kubernetes", "Terraform"}
 

SQL language - Declarative or Imperative

SQL - Select * from employee;  // command  - Declarative

Result  - table with all records


HTML - <h1>  text  </h1> // - Declarative

   ----------------------------------------------------------------------------------------------

  Docker Compose - Declarative Syntax - YAML format


What yaml - YAML (YAML Ain't Markup Language) is a human-friendly data  standard often used for configuration files in Devops

Key-Value Pairs (Mappings/Dictionaries):
Represented as key: value.
Indentation defines hierarchy. 



Yaml format
 user:
          id: 123 # single value
          details:  # dictionary 
            name: Jane Doe
            email: jane.doe@example.com
          roles: # List or Array
            - admin
            - editor

JSON format  -
user = {
"id": 123, 
"details": {"name": "jane" , "email": "jane.doe@.." },
roles: ["admin", "editor"]
}



Devop tools  use yaml format to define their sepecifications or APIs


docker compose yaml file

version: "1.29.2"

services:  // docker container run
  - containers

networks:  // docker network create mynetwork
  - "mynetwork"

volumes: // docker volume create mysqldata
  - mysqldata  



Python - connect(database_url="mysql://information of dbcontainert:3306/mydatadb",
 username="{POSTGRES_USER}" -> "user"
 password="password"
)


scenario : run a nginx container using docker compose

docker run --name web1 -d -p 8990:80 - run suing docker compose

├── nginx-docker-compose-demo
│   ├── app
│   │   └── index.html
│   └── docker-compose.yaml

                                                      

Docker compose practice example

 - node project 

 - mysql project

python - mysql 



















